{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "import duckdb\n",
    "import pandas as pd\n",
    "from functools import reduce\n",
    "import numpy as np\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "conn = duckdb.connect('db/db.duckdb', read_only=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['end_year_test', 'end_year_train', 'end_year_validation', 'labels_train', 'num_votes_test', 'num_votes_train', 'num_votes_validation', 'original_title_test', 'original_title_train', 'original_title_validation', 'primary_title_test', 'primary_title_train', 'primary_title_validation', 'runtime_minutes_test', 'runtime_minutes_train', 'runtime_minutes_validation', 'start_year_test', 'start_year_train', 'start_year_validation', 'user_ratings_test', 'user_ratings_train', 'user_ratings_validation']\n"
     ]
    }
   ],
   "source": [
    "print(conn.execute('PRAGMA show_tables').fetchdf()['name'].tolist())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Catalog Error: Table with name user_ratings_train does not exist!\nDid you mean \"num_votes_train\"?\nLINE 1: SELECT * FROM user_ratings_train\n                      ^",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Input \u001b[1;32mIn [17]\u001b[0m, in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m val \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muser_ratings\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m----> 2\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mconn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mSELECT * FROM \u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mval\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m_train\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mfetchdf())\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(conn\u001b[38;5;241m.\u001b[39mexecute(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSELECT * FROM \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mval\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_test\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39mfetchdf())\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(conn\u001b[38;5;241m.\u001b[39mexecute(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSELECT * FROM \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mval\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_validation\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39mfetchdf())\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Catalog Error: Table with name user_ratings_train does not exist!\nDid you mean \"num_votes_train\"?\nLINE 1: SELECT * FROM user_ratings_train\n                      ^"
     ]
    }
   ],
   "source": [
    "val = \"user_ratings\"\n",
    "print(conn.execute(f'SELECT * FROM {val}_train').fetchdf())\n",
    "print(conn.execute(f'SELECT * FROM {val}_test').fetchdf())\n",
    "print(conn.execute(f'SELECT * FROM {val}_validation').fetchdf())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "conn.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Connect to duckdb and create dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['end_year_test', 'end_year_train', 'end_year_validation', 'labels_train', 'num_votes_test', 'num_votes_train', 'num_votes_validation', 'original_title_test', 'original_title_train', 'original_title_validation', 'primary_title_test', 'primary_title_train', 'primary_title_validation', 'runtime_minutes_test', 'runtime_minutes_train', 'runtime_minutes_validation', 'start_year_test', 'start_year_train', 'start_year_validation', 'user_ratings_test', 'user_ratings_train', 'user_ratings_validation']\n"
     ]
    }
   ],
   "source": [
    "conn = duckdb.connect('db/db.duckdb', read_only=False)\n",
    "# Create dataframe from all tables in the db\n",
    "print(conn.execute('PRAGMA show_tables').fetchdf()['name'].tolist())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Any NaN values in the df: True\n"
     ]
    }
   ],
   "source": [
    "merged_df = conn.execute('''SELECT end_year_train.tconst, end_year_train.end_year, labels_train.labels, num_votes_train.num_votes, original_title_train.original_title, primary_title_train.primary_title, runtime_minutes_train.runtime_minutes, start_year_train.start_year, user_ratings_train.user_ratings  \n",
    "                         FROM end_year_train \n",
    "                         INNER JOIN labels_train ON labels_train.tconst = end_year_train.tconst\n",
    "                         INNER JOIN num_votes_train ON end_year_train.tconst = num_votes_train.tconst\n",
    "                         INNER JOIN original_title_train ON end_year_train.tconst = original_title_train.tconst\n",
    "                         INNER JOIN primary_title_train ON end_year_train.tconst = primary_title_train.tconst\n",
    "                         INNER JOIN runtime_minutes_train ON end_year_train.tconst = runtime_minutes_train.tconst\n",
    "                         INNER JOIN start_year_train ON end_year_train.tconst = start_year_train.tconst\n",
    "                         FULL OUTER JOIN user_ratings_train ON end_year_train.tconst = user_ratings_train.tconst\n",
    "                         ''').fetchdf()\n",
    "\n",
    "tconst_order = conn.execute('SELECT tconst FROM end_year_train').fetchdf()\n",
    "tconst_order['order'] = range(len(tconst_order))\n",
    "merged_df = merged_df.merge(tconst_order, on='tconst').sort_values(by=['order']).drop('order', axis=1)\n",
    "print(f\"Any NaN values in the df: {merged_df.isnull().values.any()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         tconst  end_year  labels  num_votes                  original_title  \\\n",
      "934   tt0010600         0    True       1898                       Die Puppe   \n",
      "881   tt0011841         0    True       5376                   Way Down East   \n",
      "0     tt0012494         0    True       5842                    Der mude Tod   \n",
      "1     tt0015163         0    True       9652                   The Navigator   \n",
      "2     tt0016220         0    True      17887        The Phantom of the Opera   \n",
      "...         ...       ...     ...        ...                             ...   \n",
      "7693  tt9625664         0   False      12951                                   \n",
      "7955  tt9741310         0   False       2464                           Slaxx   \n",
      "7694  tt9742392         0   False       1719                         Kindred   \n",
      "7695  tt9850386         0    True       4144                                   \n",
      "7696  tt9911196         0    True       3242  De beentjes van Sint-Hildegard   \n",
      "\n",
      "                                      primary_title  runtime_minutes  \\\n",
      "934                                        The Doll               66   \n",
      "881                                   Way Down East              145   \n",
      "0                                           Destiny               97   \n",
      "1                                     The Navigator               59   \n",
      "2                          The Phantom of the Opera               93   \n",
      "...                                             ...              ...   \n",
      "7693                                  Trauma Center               87   \n",
      "7955                                          Slaxx               77   \n",
      "7694                                        Kindred              101   \n",
      "7695  The Bee Gees: How Can You Mend a Broken Heart              111   \n",
      "7696                            The Marriage Escape              103   \n",
      "\n",
      "      start_year  user_ratings  \n",
      "934         1919           NaN  \n",
      "881         1920      7.000000  \n",
      "0           1921      9.000000  \n",
      "1           1924      8.400000  \n",
      "2           1925      7.250000  \n",
      "...          ...           ...  \n",
      "7693        2019      3.375000  \n",
      "7955        2020           NaN  \n",
      "7694        2020      6.333333  \n",
      "7695        2020      8.750000  \n",
      "7696        2020      8.000000  \n",
      "\n",
      "[7959 rows x 9 columns]\n"
     ]
    }
   ],
   "source": [
    "print(merged_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tables = conn.execute('PRAGMA show_tables').fetchdf()['name'].tolist()\n",
    "# frames = []\n",
    "# for i in tables:\n",
    "#     if not i.startswith('test') and not i.startswith('validation'):\n",
    "#         frames.append(conn.execute(f\"SELECT * FROM {i}\").fetchdf())\n",
    "# merged_df = reduce(lambda  left,right: pd.merge(left,right,on=['tconst'], how='outer'), frames)\n",
    "# print(f\"Any NaN values in the df: {merged_df.isnull().values.any()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add value to indicate wether the title has been changed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "renamed = []\n",
    "for i in range(len(merged_df)):\n",
    "    curr_original = merged_df.iloc[i]['original_title']\n",
    "    curr_primary = merged_df.iloc[i]['primary_title']\n",
    "    if curr_original != \"\" and curr_primary != curr_original:\n",
    "        renamed.append(1)\n",
    "    else:\n",
    "        renamed.append(0)\n",
    "    \n",
    "merged_df['renamed'] = renamed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transform dataframe to ML suited data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transform dataframe to array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tconst              object\n",
      "end_year             int32\n",
      "labels                bool\n",
      "num_votes            int32\n",
      "original_title      object\n",
      "primary_title       object\n",
      "runtime_minutes      int32\n",
      "start_year           int32\n",
      "user_ratings       float32\n",
      "renamed              int64\n",
      "dtype: object\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\casbe\\AppData\\Local\\Temp/ipykernel_16680/798717796.py:2: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only\n",
      "  merged_df = merged_df.drop('original_title', 1)\n",
      "C:\\Users\\casbe\\AppData\\Local\\Temp/ipykernel_16680/798717796.py:3: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only\n",
      "  merged_df = merged_df.drop('primary_title', 1)\n",
      "C:\\Users\\casbe\\AppData\\Local\\Temp/ipykernel_16680/798717796.py:4: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only\n",
      "  merged_df = merged_df.drop('tconst', 1)\n"
     ]
    }
   ],
   "source": [
    "print(merged_df.dtypes)\n",
    "merged_df = merged_df.drop('original_title', 1)\n",
    "merged_df = merged_df.drop('primary_title', 1)\n",
    "merged_df = merged_df.drop('tconst', 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      end_year  labels  num_votes  runtime_minutes  start_year  user_ratings  \\\n",
      "934          0    True       1898               66        1919           NaN   \n",
      "881          0    True       5376              145        1920      7.000000   \n",
      "0            0    True       5842               97        1921      9.000000   \n",
      "1            0    True       9652               59        1924      8.400000   \n",
      "2            0    True      17887               93        1925      7.250000   \n",
      "...        ...     ...        ...              ...         ...           ...   \n",
      "7693         0   False      12951               87        2019      3.375000   \n",
      "7955         0   False       2464               77        2020           NaN   \n",
      "7694         0   False       1719              101        2020      6.333333   \n",
      "7695         0    True       4144              111        2020      8.750000   \n",
      "7696         0    True       3242              103        2020      8.000000   \n",
      "\n",
      "      renamed  \n",
      "934         1  \n",
      "881         0  \n",
      "0           1  \n",
      "1           0  \n",
      "2           0  \n",
      "...       ...  \n",
      "7693        0  \n",
      "7955        0  \n",
      "7694        0  \n",
      "7695        0  \n",
      "7696        1  \n",
      "\n",
      "[7959 rows x 7 columns]\n"
     ]
    }
   ],
   "source": [
    "print(merged_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\casbe\\AppData\\Local\\Temp/ipykernel_16680/1788728139.py:1: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only\n",
      "  merged_df_without_ratings = merged_df.drop('user_ratings', 1)\n"
     ]
    }
   ],
   "source": [
    "merged_df_without_ratings = merged_df.drop('user_ratings', 1)\n",
    "merged_df_with_ratings = merged_df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\casbe\\AppData\\Local\\Temp/ipykernel_16680/908182688.py:3: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only\n",
      "  merged_df_without_ratings = merged_df_without_ratings.drop('labels', 1)\n"
     ]
    }
   ],
   "source": [
    "labels_without_ratings = merged_df_without_ratings['labels']\n",
    "labels_without_ratings = np.array(labels_without_ratings.astype('int').tolist())\n",
    "merged_df_without_ratings = merged_df_without_ratings.drop('labels', 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\casbe\\AppData\\Local\\Temp/ipykernel_16680/2110107514.py:3: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only\n",
      "  merged_df_with_ratings = merged_df_with_ratings.drop('labels', 1)\n"
     ]
    }
   ],
   "source": [
    "labels_with_ratings = merged_df_with_ratings['labels']\n",
    "labels_with_ratings = np.array(labels_with_ratings.astype('int').tolist())\n",
    "merged_df_with_ratings = merged_df_with_ratings.drop('labels', 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7959\n",
      "7959\n",
      "5540\n",
      "5540\n"
     ]
    }
   ],
   "source": [
    "print(len(merged_df_without_ratings))\n",
    "print(len(labels_without_ratings))\n",
    "print(len(merged_df_with_ratings))\n",
    "print(len(labels_with_ratings))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_array_without_ratings = merged_df_without_ratings.to_numpy()\n",
    "full_array_with_ratings = merged_df_with_ratings.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7959\n",
      "5540\n"
     ]
    }
   ],
   "source": [
    "print(len(full_array_without_ratings))\n",
    "print(len(full_array_with_ratings))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train / test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6367, 5)\n",
      "(6367,)\n",
      "(1592, 5)\n",
      "(1592,)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "standardizer = StandardScaler()\n",
    "full_array_without_ratings = standardizer.fit_transform(full_array_without_ratings)\n",
    "\n",
    "split = int(len(full_array_without_ratings) * 0.8)\n",
    "\n",
    "X_train_without_ratings = full_array_without_ratings[:split]\n",
    "y_train_without_ratings = labels_without_ratings[:split]\n",
    "\n",
    "X_test_without_ratings = full_array_without_ratings[split:]\n",
    "y_test_without_ratings = labels_without_ratings[split:]\n",
    "\n",
    "print(X_train_without_ratings.shape)\n",
    "print(y_train_without_ratings.shape)\n",
    "print(X_test_without_ratings.shape)\n",
    "print(y_test_without_ratings.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4432, 6)\n",
      "(4432,)\n",
      "(1108, 6)\n",
      "(1108,)\n"
     ]
    }
   ],
   "source": [
    "full_array_with_ratings = standardizer.fit_transform(full_array_with_ratings)\n",
    "\n",
    "split = int(len(full_array_with_ratings) * 0.8)\n",
    "\n",
    "X_train_with_ratings = full_array_with_ratings[:split]\n",
    "y_train_with_ratings = labels_with_ratings[:split]\n",
    "\n",
    "X_test_with_ratings = full_array_with_ratings[split:]\n",
    "y_test_with_ratings = labels_with_ratings[split:]\n",
    "\n",
    "print(X_train_with_ratings.shape)\n",
    "print(y_train_with_ratings.shape)\n",
    "print(X_test_with_ratings.shape)\n",
    "print(y_test_with_ratings.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.32995896 -0.24666328  1.56608671  0.19767778  0.17880145 -0.37905528]\n",
      " [-0.32995896 -0.2430555  -0.37551326  0.19935816  1.30594179  2.63813762]\n",
      " [-0.32995896 -0.21355846 -1.91261324  0.20439929  0.96779947 -0.37905528]\n",
      " ...\n",
      " [-0.32995896 -0.2882843   1.9705867   0.33714923 -1.69976574  2.63813762]\n",
      " [-0.32995896 -0.25399496 -0.49686326  0.33714923 -0.38476872 -0.37905528]\n",
      " [-0.32995896  2.53056486  0.51438673  0.33882961  1.05142647  2.63813762]]\n"
     ]
    }
   ],
   "source": [
    "print(X_train_with_ratings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Machine Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X_train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [46]\u001b[0m, in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlinear_model\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m LogisticRegression\n\u001b[0;32m      2\u001b[0m model \u001b[38;5;241m=\u001b[39m LogisticRegression()\n\u001b[1;32m----> 3\u001b[0m model\u001b[38;5;241m.\u001b[39mfit(\u001b[43mX_train\u001b[49m, y_train)\n\u001b[0;32m      5\u001b[0m predictions \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mpredict(X_test)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'X_train' is not defined"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "model = LogisticRegression()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "predictions = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True Positive(TP)  =  483\n",
      "False Positive(FP) =  82\n",
      "True Negative(TN)  =  492\n",
      "False Negative(FN) =  53\n",
      "Accuracy of the binary classification = 0.878\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "cm = confusion_matrix(y_test, predictions)\n",
    "\n",
    "TN, FP, FN, TP = confusion_matrix(y_test, predictions).ravel()\n",
    "\n",
    "print('True Positive(TP)  = ', TP)\n",
    "print('False Positive(FP) = ', FP)\n",
    "print('True Negative(TN)  = ', TN)\n",
    "print('False Negative(FN) = ', FN)\n",
    "\n",
    "accuracy =  (TP+TN) /(TP+FP+TN+FN)\n",
    "\n",
    "print('Accuracy of the binary classification = {:0.3f}'.format(accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras.models import Sequential   # importing Sequential model\n",
    "from keras.layers import Dense        # importing Dense layers\n",
    "import keras.optimizers\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "basic_model = Sequential()\n",
    "# Adding layers to the model\n",
    "# First layers: 16 neurons/perceptrons that takes the input and uses 'sigmoid' activation function.\n",
    "basic_model.add(Dense(units = 16 , activation = 'sigmoid', input_shape = (6,))) \n",
    "# Second layer: 1 neuron/perceptron that takes the input from the 1st layers and gives output as 0 or 1.Activation used is 'Hard Sigmoid'\n",
    "basic_model.add(Dense(1, activation = 'hard_sigmoid'))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "sgd = tf.keras.optimizers.SGD(learning_rate=0.5, momentum=0.9, nesterov=True)\n",
    "basic_model.compile(loss = 'binary_crossentropy', optimizer = 'sgd', metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "139/139 [==============================] - 0s 856us/step - loss: 0.6814 - accuracy: 0.4497\n",
      "Epoch 2/50\n",
      "139/139 [==============================] - 0s 679us/step - loss: 0.6559 - accuracy: 0.4876\n",
      "Epoch 3/50\n",
      "139/139 [==============================] - 0s 664us/step - loss: 0.6310 - accuracy: 0.5851\n",
      "Epoch 4/50\n",
      "139/139 [==============================] - 0s 643us/step - loss: 0.6114 - accuracy: 0.7014\n",
      "Epoch 5/50\n",
      "139/139 [==============================] - 0s 653us/step - loss: 0.5963 - accuracy: 0.7614\n",
      "Epoch 6/50\n",
      "139/139 [==============================] - 0s 713us/step - loss: 0.5868 - accuracy: 0.7941\n",
      "Epoch 7/50\n",
      "139/139 [==============================] - 0s 767us/step - loss: 0.5803 - accuracy: 0.8167\n",
      "Epoch 8/50\n",
      "139/139 [==============================] - 0s 820us/step - loss: 0.5754 - accuracy: 0.8266\n",
      "Epoch 9/50\n",
      "139/139 [==============================] - 0s 677us/step - loss: 0.5715 - accuracy: 0.8372\n",
      "Epoch 10/50\n",
      "139/139 [==============================] - 0s 642us/step - loss: 0.5684 - accuracy: 0.8429\n",
      "Epoch 11/50\n",
      "139/139 [==============================] - 0s 665us/step - loss: 0.5659 - accuracy: 0.8470\n",
      "Epoch 12/50\n",
      "139/139 [==============================] - 0s 679us/step - loss: 0.5640 - accuracy: 0.8510\n",
      "Epoch 13/50\n",
      "139/139 [==============================] - 0s 711us/step - loss: 0.5623 - accuracy: 0.8517\n",
      "Epoch 14/50\n",
      "139/139 [==============================] - 0s 673us/step - loss: 0.5609 - accuracy: 0.8551\n",
      "Epoch 15/50\n",
      "139/139 [==============================] - 0s 700us/step - loss: 0.5597 - accuracy: 0.8571\n",
      "Epoch 16/50\n",
      "139/139 [==============================] - 0s 754us/step - loss: 0.5586 - accuracy: 0.8596\n",
      "Epoch 17/50\n",
      "139/139 [==============================] - 0s 757us/step - loss: 0.5576 - accuracy: 0.8619\n",
      "Epoch 18/50\n",
      "139/139 [==============================] - 0s 698us/step - loss: 0.5567 - accuracy: 0.8632\n",
      "Epoch 19/50\n",
      "139/139 [==============================] - 0s 659us/step - loss: 0.5559 - accuracy: 0.8652\n",
      "Epoch 20/50\n",
      "139/139 [==============================] - 0s 642us/step - loss: 0.5552 - accuracy: 0.8682\n",
      "Epoch 21/50\n",
      "139/139 [==============================] - 0s 659us/step - loss: 0.5546 - accuracy: 0.8693\n",
      "Epoch 22/50\n",
      "139/139 [==============================] - 0s 641us/step - loss: 0.5541 - accuracy: 0.8686\n",
      "Epoch 23/50\n",
      "139/139 [==============================] - 0s 654us/step - loss: 0.5535 - accuracy: 0.8688\n",
      "Epoch 24/50\n",
      "139/139 [==============================] - 0s 661us/step - loss: 0.5530 - accuracy: 0.8695\n",
      "Epoch 25/50\n",
      "139/139 [==============================] - 0s 679us/step - loss: 0.5527 - accuracy: 0.8704\n",
      "Epoch 26/50\n",
      "139/139 [==============================] - 0s 747us/step - loss: 0.5524 - accuracy: 0.8704\n",
      "Epoch 27/50\n",
      "139/139 [==============================] - 0s 757us/step - loss: 0.5521 - accuracy: 0.8702\n",
      "Epoch 28/50\n",
      "139/139 [==============================] - 0s 707us/step - loss: 0.5518 - accuracy: 0.8707\n",
      "Epoch 29/50\n",
      "139/139 [==============================] - 0s 668us/step - loss: 0.5516 - accuracy: 0.8707\n",
      "Epoch 30/50\n",
      "139/139 [==============================] - 0s 650us/step - loss: 0.5515 - accuracy: 0.8700\n",
      "Epoch 31/50\n",
      "139/139 [==============================] - 0s 648us/step - loss: 0.5513 - accuracy: 0.8704\n",
      "Epoch 32/50\n",
      "139/139 [==============================] - 0s 658us/step - loss: 0.5513 - accuracy: 0.8698\n",
      "Epoch 33/50\n",
      "139/139 [==============================] - 0s 655us/step - loss: 0.5512 - accuracy: 0.8698\n",
      "Epoch 34/50\n",
      "139/139 [==============================] - 0s 685us/step - loss: 0.5511 - accuracy: 0.8700\n",
      "Epoch 35/50\n",
      "139/139 [==============================] - 0s 749us/step - loss: 0.5511 - accuracy: 0.8700\n",
      "Epoch 36/50\n",
      "139/139 [==============================] - 0s 753us/step - loss: 0.5511 - accuracy: 0.8702\n",
      "Epoch 37/50\n",
      "139/139 [==============================] - 0s 728us/step - loss: 0.5511 - accuracy: 0.8700\n",
      "Epoch 38/50\n",
      "139/139 [==============================] - 0s 656us/step - loss: 0.5510 - accuracy: 0.8700\n",
      "Epoch 39/50\n",
      "139/139 [==============================] - 0s 684us/step - loss: 0.5510 - accuracy: 0.8698\n",
      "Epoch 40/50\n",
      "139/139 [==============================] - 0s 649us/step - loss: 0.5510 - accuracy: 0.8700\n",
      "Epoch 41/50\n",
      "139/139 [==============================] - 0s 655us/step - loss: 0.5510 - accuracy: 0.8698\n",
      "Epoch 42/50\n",
      "139/139 [==============================] - 0s 725us/step - loss: 0.5510 - accuracy: 0.8700\n",
      "Epoch 43/50\n",
      "139/139 [==============================] - 0s 764us/step - loss: 0.5510 - accuracy: 0.8700\n",
      "Epoch 44/50\n",
      "139/139 [==============================] - 0s 739us/step - loss: 0.5510 - accuracy: 0.8700\n",
      "Epoch 45/50\n",
      "139/139 [==============================] - 0s 708us/step - loss: 0.5510 - accuracy: 0.8700\n",
      "Epoch 46/50\n",
      "139/139 [==============================] - 0s 725us/step - loss: 0.5510 - accuracy: 0.8700\n",
      "Epoch 47/50\n",
      "139/139 [==============================] - 0s 660us/step - loss: 0.5510 - accuracy: 0.8698\n",
      "Epoch 48/50\n",
      "139/139 [==============================] - 0s 674us/step - loss: 0.5510 - accuracy: 0.8698\n",
      "Epoch 49/50\n",
      "139/139 [==============================] - 0s 717us/step - loss: 0.5510 - accuracy: 0.8698\n",
      "Epoch 50/50\n",
      "139/139 [==============================] - 0s 664us/step - loss: 0.5510 - accuracy: 0.8698\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7ffb234e7580>"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "basic_model.compile(optimizer='adam',\n",
    "              loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
    "              metrics=['accuracy'])\n",
    "basic_model.fit(X_train, y_train, epochs=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "52/52 [==============================] - 0s 521us/step - loss: 0.6618 - accuracy: 0.6757\n",
      "Loss =  0.6617915630340576\n",
      "Accuracy =  0.6757082343101501\n"
     ]
    }
   ],
   "source": [
    "# Test, Loss and accuracy\n",
    "loss_and_metrics = basic_model.evaluate(X_test, y_test)\n",
    "print('Loss = ',loss_and_metrics[0])\n",
    "print('Accuracy = ',loss_and_metrics[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Keras 2 without ratings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras.models import Sequential   # importing Sequential model\n",
    "from keras.layers import Dense        # importing Dense layers\n",
    "import keras.optimizers\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.33100304 -0.22659344 -1.53571186  0.19837533  2.61118194]\n",
      " [-0.33100304 -0.19467699  1.53458386  0.20005195 -0.38296834]\n",
      " [-0.33100304 -0.19040066 -0.33091227  0.20172858  2.61118194]\n",
      " ...\n",
      " [-0.33100304 -0.2334943  -1.18593133  0.33250532 -0.38296834]\n",
      " [-0.33100304  0.32970536  0.01886825  0.33921182 -0.38296834]\n",
      " [ 3.0227923  -0.24401076 -0.13658976 -3.01906779  2.61118194]]\n"
     ]
    }
   ],
   "source": [
    "print(X_train_without_ratings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "keras2model_without_ratings = keras.Sequential([\n",
    "    keras.layers.Flatten(input_shape=(5,)),\n",
    "    keras.layers.Dense(16, activation=tf.nn.relu),\n",
    "    keras.layers.Dense(32, activation=tf.nn.relu),\n",
    "    keras.layers.Dense(16, activation=tf.nn.relu),\n",
    "    keras.layers.Dense(1, activation=tf.nn.sigmoid),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "6367/6367 [==============================] - 4s 555us/step - loss: 0.6114 - accuracy: 0.6766\n",
      "Epoch 2/10\n",
      "6367/6367 [==============================] - 4s 550us/step - loss: 0.5968 - accuracy: 0.6871\n",
      "Epoch 3/10\n",
      "6367/6367 [==============================] - 4s 559us/step - loss: 0.5929 - accuracy: 0.6890\n",
      "Epoch 4/10\n",
      "6367/6367 [==============================] - 4s 551us/step - loss: 0.5862 - accuracy: 0.6972\n",
      "Epoch 5/10\n",
      "6367/6367 [==============================] - 3s 548us/step - loss: 0.5739 - accuracy: 0.7116\n",
      "Epoch 6/10\n",
      "6367/6367 [==============================] - 4s 552us/step - loss: 0.5679 - accuracy: 0.7192\n",
      "Epoch 7/10\n",
      "6367/6367 [==============================] - 4s 556us/step - loss: 0.5642 - accuracy: 0.7168\n",
      "Epoch 8/10\n",
      "6367/6367 [==============================] - 4s 554us/step - loss: 0.5608 - accuracy: 0.7196\n",
      "Epoch 9/10\n",
      "6367/6367 [==============================] - 3s 548us/step - loss: 0.5557 - accuracy: 0.7200\n",
      "Epoch 10/10\n",
      "6367/6367 [==============================] - 4s 550us/step - loss: 0.5556 - accuracy: 0.7185\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x206ee2cfbb0>"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keras2model_without_ratings.compile(optimizer='adam',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "keras2model_without_ratings.fit(X_train_without_ratings, y_train_without_ratings, epochs=10, batch_size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50/50 [==============================] - 0s 490us/step - loss: 0.5821 - accuracy: 0.6928\n",
      "Loss =  0.5821307897567749\n",
      "Accuracy =  0.6928392052650452\n"
     ]
    }
   ],
   "source": [
    "# Test, Loss and accuracy\n",
    "loss_and_metrics = keras2model_without_ratings.evaluate(X_test_without_ratings, y_test_without_ratings)\n",
    "print('Loss without ratings = ',loss_and_metrics[0])\n",
    "print('Accuracy without ratings = ',loss_and_metrics[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Keras 2 with ratings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras.models import Sequential   # importing Sequential model\n",
    "from keras.layers import Dense        # importing Dense layers\n",
    "import keras.optimizers\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "keras2model_with_ratings = keras.Sequential([\n",
    "    keras.layers.Flatten(input_shape=(6,)),\n",
    "    keras.layers.Dense(16, activation=tf.nn.relu),\n",
    "    keras.layers.Dense(32, activation=tf.nn.relu),\n",
    "    keras.layers.Dense(16, activation=tf.nn.relu),\n",
    "    keras.layers.Dense(1, activation=tf.nn.sigmoid),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "4432/4432 [==============================] - 3s 544us/step - loss: 0.3620 - accuracy: 0.8538\n",
      "Epoch 2/10\n",
      "4432/4432 [==============================] - 2s 544us/step - loss: 0.3185 - accuracy: 0.8730\n",
      "Epoch 3/10\n",
      "4432/4432 [==============================] - 2s 544us/step - loss: 0.3130 - accuracy: 0.8730\n",
      "Epoch 4/10\n",
      "4432/4432 [==============================] - 2s 542us/step - loss: 0.3047 - accuracy: 0.8779\n",
      "Epoch 5/10\n",
      "4432/4432 [==============================] - 2s 544us/step - loss: 0.2975 - accuracy: 0.8813\n",
      "Epoch 6/10\n",
      "4432/4432 [==============================] - 2s 545us/step - loss: 0.2943 - accuracy: 0.8755\n",
      "Epoch 7/10\n",
      "4432/4432 [==============================] - 2s 547us/step - loss: 0.2917 - accuracy: 0.8806\n",
      "Epoch 8/10\n",
      "4432/4432 [==============================] - 2s 551us/step - loss: 0.2873 - accuracy: 0.8838\n",
      "Epoch 9/10\n",
      "4432/4432 [==============================] - 2s 543us/step - loss: 0.2863 - accuracy: 0.8824\n",
      "Epoch 10/10\n",
      "4432/4432 [==============================] - 2s 559us/step - loss: 0.2840 - accuracy: 0.8815\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x206ecf7a8c0>"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keras2model_with_ratings.compile(optimizer='adam',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "keras2model_with_ratings.fit(X_train_with_ratings, y_train_with_ratings, epochs=10, batch_size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "35/35 [==============================] - 0s 471us/step - loss: 0.2770 - accuracy: 0.8953\n",
      "Loss =  0.277017205953598\n",
      "Accuracy =  0.8953068852424622\n"
     ]
    }
   ],
   "source": [
    "# Test, Loss and accuracy\n",
    "loss_and_metrics = keras2model_with_ratings.evaluate(X_test_with_ratings, y_test_with_ratings)\n",
    "print('Loss = ',loss_and_metrics[0])\n",
    "print('Accuracy = ',loss_and_metrics[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Random forest classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Import the model we are using\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "# Instantiate model with 1000 decision trees\n",
    "rf = RandomForestClassifier(n_estimators = 1000, random_state = 42)\n",
    "# Train the model on training data\n",
    "rf.fit(X_train, y_train);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Use the forest's predict method on the test data\n",
    "predictions = rf.predict(X_test)\n",
    "# Test, Loss and accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8846846846846846"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "accuracy_score(y_test, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict on test/validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "import duckdb\n",
    "import pandas as pd\n",
    "from functools import reduce\n",
    "import numpy as np\n",
    "import os\n",
    "# conn = duckdb.connect('db/db.duckdb', read_only=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tables = conn.execute('PRAGMA show_tables').fetchdf()['name'].tolist()\n",
    "# frames = []\n",
    "# for i in tables:\n",
    "#     if i.startswith('test'):\n",
    "#         frames.append(conn.execute(f\"SELECT * FROM {i}\").fetchdf())\n",
    "# test_merged_df = reduce(lambda  left,right: pd.merge(left,right,on=['tconst'], how='outer'), frames)\n",
    "# print(f\"Any NaN values in the df: {test_merged_df.isnull().values.any()}\")\n",
    "# print(test_merged_df.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Any NaN values in the df: True\n",
      "tconst               0\n",
      "end_year             0\n",
      "num_votes            0\n",
      "original_title       0\n",
      "primary_title        0\n",
      "runtime_minutes      0\n",
      "start_year           0\n",
      "user_ratings       343\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "test_merged_df = conn.execute('''SELECT end_year_test.tconst, end_year_test.end_year, num_votes_test.num_votes, original_title_test.original_title, primary_title_test.primary_title, runtime_minutes_test.runtime_minutes, start_year_test.start_year, user_ratings_test.user_ratings  \n",
    "                         FROM end_year_test \n",
    "                         INNER JOIN num_votes_test ON end_year_test.tconst = num_votes_test.tconst\n",
    "                         INNER JOIN original_title_test ON end_year_test.tconst = original_title_test.tconst\n",
    "                         INNER JOIN primary_title_test ON end_year_test.tconst = primary_title_test.tconst\n",
    "                         INNER JOIN runtime_minutes_test ON end_year_test.tconst = runtime_minutes_test.tconst\n",
    "                         INNER JOIN start_year_test ON end_year_test.tconst = start_year_test.tconst\n",
    "                         FULL OUTER JOIN user_ratings_test ON end_year_test.tconst = user_ratings_test.tconst\n",
    "                         ''').fetchdf()\n",
    "\n",
    "tconst_order = conn.execute('SELECT tconst FROM end_year_test').fetchdf()\n",
    "tconst_order['order'] = range(len(tconst_order))\n",
    "test_merged_df = test_merged_df.merge(tconst_order, on='tconst').sort_values(by=['order']).drop('order', axis=1)\n",
    "print(f\"Any NaN values in the df: {test_merged_df.isnull().values.any()}\")\n",
    "print(test_merged_df.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# tables = conn.execute('PRAGMA show_tables').fetchdf()['name'].tolist()\n",
    "# frames = []\n",
    "# for i in tables:\n",
    "#     if i.startswith('validation'):\n",
    "#         frames.append(conn.execute(f\"SELECT * FROM {i}\").fetchdf())\n",
    "# validation_merged_df = reduce(lambda  left,right: pd.merge(left,right,on=['tconst'], how='outer'), frames)\n",
    "# print(f\"Any NaN values in the df: {validation_merged_df.isnull().values.any()}\")\n",
    "# print(validation_merged_df.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Any NaN values in the df: True\n",
      "tconst               0\n",
      "end_year             0\n",
      "num_votes            0\n",
      "original_title       0\n",
      "primary_title        0\n",
      "runtime_minutes      0\n",
      "start_year           0\n",
      "user_ratings       306\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "validation_merged_df = conn.execute('''SELECT end_year_validation.tconst, end_year_validation.end_year, num_votes_validation.num_votes, original_title_validation.original_title, primary_title_validation.primary_title, runtime_minutes_validation.runtime_minutes, start_year_validation.start_year, user_ratings_validation.user_ratings  \n",
    "                         FROM end_year_validation \n",
    "                         INNER JOIN num_votes_validation ON end_year_validation.tconst = num_votes_validation.tconst\n",
    "                         INNER JOIN original_title_validation ON end_year_validation.tconst = original_title_validation.tconst\n",
    "                         INNER JOIN primary_title_validation ON end_year_validation.tconst = primary_title_validation.tconst\n",
    "                         INNER JOIN runtime_minutes_validation ON end_year_validation.tconst = runtime_minutes_validation.tconst\n",
    "                         INNER JOIN start_year_validation ON end_year_validation.tconst = start_year_validation.tconst\n",
    "                         FULL OUTER JOIN user_ratings_validation ON end_year_validation.tconst = user_ratings_validation.tconst\n",
    "                         ''').fetchdf()\n",
    "\n",
    "tconst_order = conn.execute('SELECT tconst FROM end_year_validation').fetchdf()\n",
    "tconst_order['order'] = range(len(tconst_order))\n",
    "validation_merged_df = validation_merged_df.merge(tconst_order, on='tconst').sort_values(by=['order']).drop('order', axis=1)\n",
    "print(f\"Any NaN values in the df: {validation_merged_df.isnull().values.any()}\")\n",
    "print(validation_merged_df.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1086\n",
      "1086\n"
     ]
    }
   ],
   "source": [
    "print(len(test_merged_df))\n",
    "bb = pd.read_csv(os.getcwd() + \"/imdb/test_hidden.csv\")\n",
    "print(len(bb))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "955\n",
      "955\n"
     ]
    }
   ],
   "source": [
    "print(len(validation_merged_df))\n",
    "dd = pd.read_csv(os.getcwd() + \"/imdb/validation_hidden.csv\")\n",
    "print(len(dd))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "renamed = []\n",
    "for i in range(len(test_merged_df)):\n",
    "    curr_original = test_merged_df.iloc[i]['original_title']\n",
    "    curr_primary = test_merged_df.iloc[i]['primary_title']\n",
    "    if curr_original != \"\" and curr_primary != curr_original:\n",
    "        renamed.append(1)\n",
    "    else:\n",
    "        renamed.append(0)\n",
    "    \n",
    "test_merged_df['renamed'] = renamed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "renamed = []\n",
    "for i in range(len(validation_merged_df)):\n",
    "    curr_original = validation_merged_df.iloc[i]['original_title']\n",
    "    curr_primary = validation_merged_df.iloc[i]['primary_title']\n",
    "    if curr_original != \"\" and curr_primary != curr_original:\n",
    "        renamed.append(1)\n",
    "    else:\n",
    "        renamed.append(0)\n",
    "    \n",
    "validation_merged_df['renamed'] = renamed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\casbe\\AppData\\Local\\Temp/ipykernel_16680/2573630175.py:1: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only\n",
      "  test_merged_df = test_merged_df.drop('original_title', 1)\n",
      "C:\\Users\\casbe\\AppData\\Local\\Temp/ipykernel_16680/2573630175.py:2: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only\n",
      "  test_merged_df = test_merged_df.drop('primary_title', 1)\n",
      "C:\\Users\\casbe\\AppData\\Local\\Temp/ipykernel_16680/2573630175.py:3: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only\n",
      "  test_merged_df = test_merged_df.drop('tconst', 1)\n"
     ]
    }
   ],
   "source": [
    "test_merged_df = test_merged_df.drop('original_title', 1)\n",
    "test_merged_df = test_merged_df.drop('primary_title', 1)\n",
    "test_merged_df = test_merged_df.drop('tconst', 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\casbe\\AppData\\Local\\Temp/ipykernel_16680/165287501.py:1: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only\n",
      "  validation_merged_df = validation_merged_df.drop('original_title', 1)\n",
      "C:\\Users\\casbe\\AppData\\Local\\Temp/ipykernel_16680/165287501.py:2: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only\n",
      "  validation_merged_df = validation_merged_df.drop('primary_title', 1)\n",
      "C:\\Users\\casbe\\AppData\\Local\\Temp/ipykernel_16680/165287501.py:3: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only\n",
      "  validation_merged_df = validation_merged_df.drop('tconst', 1)\n"
     ]
    }
   ],
   "source": [
    "validation_merged_df = validation_merged_df.drop('original_title', 1)\n",
    "validation_merged_df = validation_merged_df.drop('primary_title', 1)\n",
    "validation_merged_df = validation_merged_df.drop('tconst', 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "743 + 343 = 1086\n",
      "649 + 306 = 955\n",
      "743 + 343 = 1086\n",
      "649 + 306 = 955\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\casbe\\AppData\\Local\\Temp/ipykernel_16680/978231876.py:13: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only\n",
      "  test_without_rating = test_without_rating.drop('user_ratings', 1)\n",
      "C:\\Users\\casbe\\AppData\\Local\\Temp/ipykernel_16680/978231876.py:18: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only\n",
      "  validation_without_rating = validation_without_rating.drop('user_ratings', 1)\n"
     ]
    }
   ],
   "source": [
    "test_with_rating_index = np.where(test_merged_df['user_ratings'].notnull())[0]\n",
    "test_without_rating_index = np.where(test_merged_df['user_ratings'].isnull())[0]\n",
    "\n",
    "validation_with_rating_index = np.where(validation_merged_df['user_ratings'].notnull())[0]\n",
    "validation_without_rating_index = np.where(validation_merged_df['user_ratings'].isnull())[0]\n",
    "\n",
    "print(f\"{len(test_with_rating_index)} + {len(test_without_rating_index)} = {len(test_merged_df)}\")\n",
    "print(f\"{len(validation_with_rating_index)} + {len(validation_without_rating_index)} = {len(validation_merged_df)}\")\n",
    "\n",
    "\n",
    "test_with_rating = test_merged_df.iloc[test_with_rating_index]\n",
    "test_without_rating = test_merged_df.iloc[test_without_rating_index]\n",
    "test_without_rating = test_without_rating.drop('user_ratings', 1)\n",
    "print(f\"{len(test_with_rating)} + {len(test_without_rating)} = {len(test_merged_df)}\")\n",
    "\n",
    "validation_with_rating = validation_merged_df.iloc[validation_with_rating_index]\n",
    "validation_without_rating = validation_merged_df.iloc[validation_without_rating_index]\n",
    "validation_without_rating = validation_without_rating.drop('user_ratings', 1)\n",
    "print(f\"{len(validation_with_rating)} + {len(validation_without_rating)} = {len(validation_merged_df)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "343\n",
      "743\n",
      "306\n",
      "649\n"
     ]
    }
   ],
   "source": [
    "full_test_array_without_ratings = test_without_rating.to_numpy()\n",
    "full_test_array_with_ratings = test_with_rating.to_numpy()\n",
    "\n",
    "full_validation_array_without_ratings = validation_without_rating.to_numpy()\n",
    "full_validation_array_with_ratings = validation_with_rating.to_numpy()\n",
    "\n",
    "print(len(full_test_array_without_ratings))\n",
    "print(len(full_test_array_with_ratings))\n",
    "print(len(full_validation_array_without_ratings))\n",
    "print(len(full_validation_array_with_ratings))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "standardizer = StandardScaler()\n",
    "full_test_array_without_ratings = standardizer.fit_transform(full_test_array_without_ratings)\n",
    "full_test_array_with_ratings = standardizer.fit_transform(full_test_array_with_ratings)\n",
    "full_validation_array_without_ratings = standardizer.fit_transform(full_validation_array_without_ratings)\n",
    "full_validation_array_with_ratings = standardizer.fit_transform(full_validation_array_with_ratings)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "343\n",
      "343\n",
      "743\n",
      "743\n",
      "306\n",
      "306\n",
      "649\n",
      "649\n"
     ]
    }
   ],
   "source": [
    "test_predictions_without_ratings = keras2model_without_ratings.predict(full_test_array_without_ratings)\n",
    "test_predictions_without_ratings = list(map(lambda x: False if x<0.5 else True, test_predictions_without_ratings))\n",
    "\n",
    "test_predictions_with_ratings = keras2model_with_ratings.predict(full_test_array_with_ratings)\n",
    "test_predictions_with_ratings = list(map(lambda x: False if x<0.5 else True, test_predictions_with_ratings))\n",
    "\n",
    "\n",
    "validation_predictions_without_ratings = keras2model_without_ratings.predict(full_validation_array_without_ratings)\n",
    "validation_predictions_without_ratings = list(map(lambda x: False if x<0.5 else True, validation_predictions_without_ratings))\n",
    "\n",
    "\n",
    "validation_predictions_with_ratings = keras2model_with_ratings.predict(full_validation_array_with_ratings)\n",
    "validation_predictions_with_ratings = list(map(lambda x: False if x<0.5 else True, validation_predictions_with_ratings))\n",
    "\n",
    "\n",
    "print(len(test_predictions_without_ratings))\n",
    "print(len(test_without_rating_index))\n",
    "print(len(test_predictions_with_ratings))\n",
    "print(len(test_with_rating_index))\n",
    "print(len(validation_predictions_without_ratings))\n",
    "print(len(validation_without_rating_index))\n",
    "print(len(validation_predictions_with_ratings))\n",
    "print(len(validation_with_rating_index))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1086\n",
      "955\n"
     ]
    }
   ],
   "source": [
    "test_predictions_without_ratings_df = pd.DataFrame(test_predictions_without_ratings, index=test_without_rating_index)\n",
    "test_predictions_with_ratings_df = pd.DataFrame(test_predictions_with_ratings, index=test_with_rating_index)\n",
    "\n",
    "validation_predictions_without_ratings_df = pd.DataFrame(validation_predictions_without_ratings, index=validation_without_rating_index)\n",
    "validation_predictions_with_ratings_df = pd.DataFrame(validation_predictions_with_ratings, index=validation_with_rating_index)\n",
    "\n",
    "\n",
    "final_test_predictions = pd.concat([test_predictions_without_ratings_df, test_predictions_with_ratings_df], axis=0).sort_index()[0].tolist()\n",
    "final_validation_predictions = pd.concat([validation_predictions_without_ratings_df, validation_predictions_with_ratings_df], axis=0).sort_index()[0].tolist()\n",
    "\n",
    "print(len(final_test_predictions))\n",
    "print(len(final_validation_predictions))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True     491\n",
      "False    464\n",
      "Name: label, dtype: int64\n",
      "False    562\n",
      "True     524\n",
      "Name: label, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "\n",
    "validation_merged_df['label'] = final_validation_predictions\n",
    "print(validation_merged_df['label'].value_counts())\n",
    "\n",
    "test_merged_df['label'] = final_test_predictions\n",
    "print(test_merged_df['label'].value_counts())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "num = 6\n",
    "with open(f'test_predictions{num}.txt', 'w') as f:\n",
    "    for item in final_test_predictions:\n",
    "        f.write(\"%s\\n\" % item)\n",
    "with open(f'validation_predictions{num}.txt', 'w') as f:\n",
    "    for item in final_validation_predictions:\n",
    "        f.write(\"%s\\n\" % item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
